import pandas as pd
import json
import os
import subprocess
import shutil
import argparse
from tqdm import tqdm

# === è·¯å¾„ä¸ç¯å¢ƒ ===
# åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼‰
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
AGENT_DIR = os.path.join(BASE_DIR, "agent/Agentless")
DATA_FILE = os.path.join(BASE_DIR, "data/Verified.csv")
OUTPUT_ROOT = os.path.join(BASE_DIR, "results/agentless/Verified")
DETAILS_DIR = os.path.join(OUTPUT_ROOT, "details")
TRAJS_DIR = os.path.join(OUTPUT_ROOT, "trajs-fix")

env = os.environ.copy()
env["PYTHONPATH"] = f"{AGENT_DIR}:{env.get('PYTHONPATH', '')}"
env["OPENAI_BASE_URL"] = "http://127.0.0.1:5000/v1"
env["OPENAI_API_KEY"] = "sk-proxy-is-working-properly"

def run_cmd(cmd, name, log_file, timeout=1200):
    """
    è¿è¡Œå‘½ä»¤ï¼Œå¸¦è¶…æ—¶æœºåˆ¶
    
    Args:
        cmd: è¦æ‰§è¡Œçš„å‘½ä»¤åˆ—è¡¨
        name: æ­¥éª¤åç§°
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤20åˆ†é’Ÿ
    """
    import signal
    from datetime import datetime
    
    print(f"  >> [STEP] {name} (è¶…æ—¶: {timeout//60}åˆ†é’Ÿ)", flush=True)
    print(f"  >> [TIME] å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    
    with open(log_file, "a") as f:
        f.write(f"\n{'='*80}\n")
        f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting: {name}\n")
        f.write(f"Command: {' '.join(cmd)}\n")
        f.write(f"{'='*80}\n")
        f.flush()
        
        try:
            # ä½¿ç”¨ subprocess.run çš„ timeout å‚æ•°
            res = subprocess.run(
                cmd, 
                env=env, 
                cwd=AGENT_DIR, 
                stdout=f, 
                stderr=subprocess.STDOUT,
                timeout=timeout
            )
            success = res.returncode == 0
        except subprocess.TimeoutExpired:
            print(f"  !! [ERROR] {name} è¶…æ—¶ï¼ˆè¶…è¿‡ {timeout//60} åˆ†é’Ÿï¼‰", flush=True)
            f.write(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] TIMEOUT: {name} exceeded {timeout} seconds\n")
            f.flush()
            success = False
        except Exception as e:
            print(f"  !! [ERROR] {name} æ‰§è¡Œå‡ºé”™: {e}", flush=True)
            f.write(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ERROR: {name} failed with exception: {e}\n")
            f.flush()
            success = False
    
    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"  >> [TIME] ç»“æŸæ—¶é—´: {end_time}", flush=True)
    
    return success

def get_jsonl_last(fpath):
    """è¯»å– JSONL æ–‡ä»¶çš„æœ€åä¸€è¡Œæœ‰æ•ˆæ•°æ®"""
    if not os.path.exists(fpath): return None
    try:
        with open(fpath, "r") as f:
            lines = [l.strip() for l in f if l.strip()]
            return json.loads(lines[-1]) if lines else None
    except:
        return None

def process_instance(row):
    """å¤„ç†å•ä¸ªå®ä¾‹çš„å®Œæ•´æµç¨‹"""
    inst_id = row['instance_id']
    orig_id = row['original_inst_id']
    
    # ç›®å½•è§„åˆ’
    inst_path = os.path.join(DETAILS_DIR, inst_id)
    traj_file = os.path.join(TRAJS_DIR, f"{inst_id}_traj.json")
    
    # æ¸…é™¤è¯¥å®ä¾‹çš„æ‰€æœ‰ç¼“å­˜ï¼Œç¡®ä¿é‡æ–°ç”Ÿæˆ
    print(f"  >> [CLEAN] Clearing all cache for instance {inst_id}", flush=True)
    
    loc_path = os.path.join(inst_path, "localization")
    rep_path = os.path.join(inst_path, "repairs")
    tst_path = os.path.join(inst_path, "tests")
    final_preds_file = os.path.join(inst_path, "all_preds.jsonl")
    
    # æ¸…é™¤æ‰€æœ‰ç¼“å­˜ç›®å½•å’Œæ–‡ä»¶
    if os.path.exists(loc_path):
        shutil.rmtree(loc_path)
        print(f"  >> [CLEAN] Removed localization cache: {loc_path}", flush=True)
    if os.path.exists(rep_path):
        shutil.rmtree(rep_path)
        print(f"  >> [CLEAN] Removed repairs cache: {rep_path}", flush=True)
    if os.path.exists(tst_path):
        shutil.rmtree(tst_path)
        print(f"  >> [CLEAN] Removed tests cache: {tst_path}", flush=True)
    if os.path.exists(final_preds_file):
        os.remove(final_preds_file)
        print(f"  >> [CLEAN] Removed final predictions file: {final_preds_file}", flush=True)
    if os.path.exists(traj_file):
        os.remove(traj_file)
        print(f"  >> [CLEAN] Removed existing traj file: {traj_file}", flush=True)
    
    # æ¸…é™¤æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦å®Œå…¨é‡æ–°å¼€å§‹ï¼‰
    log_file = os.path.join(inst_path, "workflow_full.log")
    if os.path.exists(log_file):
        os.remove(log_file)
        print(f"  >> [CLEAN] Removed log file: {log_file}", flush=True)
    
    # æŒä¹…åŒ–ç´¢å¼•ç›®å½• (ä¿®å¤ EmbeddingIndex çš„ NoneType æŠ¥é”™)
    index_dir = os.path.join(loc_path, "retrieval", "index")
    os.makedirs(index_dir, exist_ok=True)

    os.makedirs(loc_path, exist_ok=True)
    os.makedirs(rep_path, exist_ok=True)
    os.makedirs(tst_path, exist_ok=True)
    log = os.path.join(inst_path, "workflow_full.log")
    
    # åˆå§‹åŒ–å˜é‡ï¼Œç”¨äº traj è®°å½•
    f_loc_file = None
    ret_file = None
    comb_file_path = None
    r_loc_file = None
    m_out = None
    
    # ä½¿ç”¨ try-finally ç¡®ä¿æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½è®°å½• traj
    try:

        # --- 1. æ•…éšœå®šä½ ---
        # 1.1 æ¨¡å‹æ–‡ä»¶å®šä½
        f_out = os.path.join(loc_path, "file_level")
        f_loc_file = None
        if run_cmd(["python", "agentless/fl/localize.py", "--file_level", "--output_folder", f_out, "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id, "--skip_existing"], "File-Level Loc", log):
            f_loc_file = os.path.join(f_out, "loc_outputs.jsonl")
        
        # 1.2 è¿‡æ»¤
        irr_out = os.path.join(loc_path, "irrelevant")
        irr_file = None
        if run_cmd(["python", "agentless/fl/localize.py", "--file_level", "--irrelevant", "--output_folder", irr_out, "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id, "--skip_existing"], "Irrelevant Filter", log):
            irr_file = os.path.join(irr_out, "loc_outputs.jsonl")
        
        # 1.3 æ£€ç´¢
        ret_out = os.path.join(loc_path, "retrieval")
        ret_file = None
        if irr_file and os.path.exists(irr_file):
            # è¡¥å…… --persist_dir ä¿®å¤æŠ¥é”™ï¼Œå¢åŠ  --chunk_size 2048 ä¿®å¤å…ƒæ•°æ®è¿‡é•¿é—®é¢˜
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 40 åˆ†é’Ÿï¼ˆ2400ç§’ï¼‰ï¼Œå› ä¸ºå¤„ç†å¤§é‡ chunks æ—¶é€Ÿåº¦ä¼šå˜æ…¢
            if run_cmd(["python", "agentless/fl/retrieve.py", "--index_type", "simple", "--filter_type", "given_files", "--filter_file", irr_file, "--output_folder", ret_out, "--target_id", orig_id, "--dataset", "princeton-nlp/SWE-bench_Verified", "--persist_dir", index_dir, "--chunk_size", "2048", "--chunk_overlap", "100"], "Embedding Retrieval", log, timeout=2400):
                ret_file = os.path.join(ret_out, "retrieve_locs.jsonl")
        else:
            print(f"  !! Skipping Embedding Retrieval (irr_file not found)", flush=True)
        
        # 1.4 åˆå¹¶
        comb_out = os.path.join(loc_path, "combined")
        comb_file_path = None
        if f_out:
            f_loc_file = os.path.join(f_out, "loc_outputs.jsonl")
            if os.path.exists(f_loc_file) and ret_file and os.path.exists(ret_file):
                # ç»•è¿‡ combine.py çš„ output_file already exists æ–­è¨€
                comb_file_path = os.path.join(comb_out, "combined_locs.jsonl")
                if os.path.exists(comb_file_path): os.remove(comb_file_path)
                if not run_cmd(["python", "agentless/fl/combine.py", "--retrieval_loc_file", ret_file, "--model_loc_file", f_loc_file, "--top_n", "3", "--output_folder", comb_out], "Combine Results", log):
                    comb_file_path = None
            else:
                print(f"  !! Skipping Combine Results (missing input files)", flush=True)
        else:
            print(f"  !! Skipping Combine Results (f_out not available)", flush=True)
        
        # 1.5 å…ƒç´ å®šä½
        r_out = os.path.join(loc_path, "related_elements")
        r_loc_file = None
        if comb_file_path and os.path.exists(comb_file_path):
            if run_cmd(["python", "agentless/fl/localize.py", "--related_level", "--output_folder", r_out, "--top_n", "3", "--compress", "--start_file", comb_file_path, "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id, "--skip_existing"], "Related-Level Loc", log):
                r_loc_file = os.path.join(r_out, "loc_outputs.jsonl")
        else:
            print(f"  !! Skipping Related-Level Loc (comb_file not found)", flush=True)
        
        # 1.6 è¡Œé‡‡æ ·
        e_out = os.path.join(loc_path, "edit_samples")
        e_loc_file = None
        if r_loc_file and os.path.exists(r_loc_file):
            if run_cmd(["python", "agentless/fl/localize.py", "--fine_grain_line_level", "--output_folder", e_out, "--top_n", "3", "--compress", "--num_samples", "4", "--temperature", "0.8", "--start_file", r_loc_file, "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id, "--skip_existing"], "Line-Level Sampling", log):
                e_loc_file = os.path.join(e_out, "loc_outputs.jsonl")
        else:
            print(f"  !! Skipping Line-Level Sampling (r_loc_file not found)", flush=True)
        
        # 1.7 æ‹†åˆ†
        m_out = os.path.join(loc_path, "merged_sets")
        if e_loc_file and os.path.exists(e_loc_file):
            if not run_cmd(["python", "agentless/fl/localize.py", "--merge", "--output_folder", m_out, "--top_n", "3", "--num_samples", "4", "--start_file", e_loc_file], "Merge Samples", log):
                m_out = None
        else:
            print(f"  !! Skipping Merge Samples (e_loc_file not found)", flush=True)
            m_out = None

        # --- 2. ä¿®å¤é˜¶æ®µ ---
        repair_success = True
        for i in range(4):
            sample_loc = os.path.join(m_out, f"loc_merged_{i}-{i}_outputs.jsonl")
            sample_rep = os.path.join(rep_path, f"sample_{i+1}")
            if not run_cmd(["python", "agentless/repair/repair.py", "--loc_file", sample_loc, "--output_folder", sample_rep, "--loc_interval", "--top_n", "3", "--max_samples", "10", "--cot", "--diff_format", "--gen_and_process", "--dataset", "princeton-nlp/SWE-bench_Verified"], f"Repair Sample {i+1}", log):
                repair_success = False
        
        if not repair_success:
            print(f"  !! Repair failed for {inst_id}. Skipping verification.")
            return False

        # --- 3. è¡¥ä¸éªŒè¯é˜¶æ®µ ---
        # 3.1 è¯†åˆ«åŸå§‹é€šè¿‡æµ‹è¯• (å›å½’æµ‹è¯•åŸºå‡†)
        pass_tests_file = os.path.join(tst_path, "passing_tests.jsonl")
        # ç¼©çŸ­ run_id ä»¥é˜²æ–‡ä»¶ç³»ç»Ÿé™åˆ¶
        short_id = inst_id[-8:]
        print(f"  >> [DEBUG] Running regression tests for original_id: {orig_id}", flush=True)
        
        # æ ‡è®°æµ‹è¯•é˜¶æ®µæ˜¯å¦æˆåŠŸ
        test_phase_success = True
        
        if not os.path.exists(pass_tests_file):
            # Find Passing Tests æ­¥éª¤ï¼šè®¾ç½®20åˆ†é’Ÿè¶…æ—¶ï¼Œå‡å°‘å¹¶è¡Œæ•°é¿å…èµ„æºç«äº‰
            if not run_cmd(
                ["python", "agentless/test/run_regression_tests.py", 
                 "--run_id", f"reg_{short_id}", 
                 "--output_file", pass_tests_file, 
                 "--dataset", "princeton-nlp/SWE-bench_Verified", 
                 "--target_id", str(orig_id),
                 "--num_workers", "4"],  # å‡å°‘å¹¶è¡Œæ•°ä»12åˆ°4ï¼Œé¿å…èµ„æºç«äº‰
                "Find Passing Tests", 
                log,
                timeout=1200  # 20åˆ†é’Ÿè¶…æ—¶
            ):
                print(f"  !! Failed to find passing tests for {inst_id}. Will skip test phase and use fallback.", flush=True)
                test_phase_success = False
        
        # 3.2 LLM ç­›é€‰å›å½’æµ‹è¯•
        repro_test_success = False
        repro_out = os.path.join(tst_path, "reproduction_samples")  # æå‰å®šä¹‰ï¼Œé¿å…æœªå®šä¹‰é”™è¯¯
        repro_final = os.path.join(repro_out, "reproduction_tests.jsonl")  # æå‰å®šä¹‰
        
        if test_phase_success:
            reg_select_out = os.path.join(tst_path, "select_regression")
            reg_tests_file = os.path.join(reg_select_out, "output.jsonl")
            if not os.path.exists(reg_tests_file):
                if not run_cmd(["python", "agentless/test/select_regression_tests.py", "--passing_tests", pass_tests_file, "--output_folder", reg_select_out, "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id], "Select Regression Tests", log):
                    test_phase_success = False
            
            # 3.3 LLM ç”Ÿæˆé‡ç°æµ‹è¯•
            if test_phase_success:
                repro_test_success = run_cmd(["python", "agentless/test/generate_reproduction_tests.py", "--max_samples", "40", "--output_folder", repro_out, "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id], "Generate Repro Tests", log)
                
                if not repro_test_success:
                    print(f"  !! Generate Repro Tests failed for {inst_id}. Will continue with fallback patch selection.", flush=True)
        
        # --- è¡¥å…¨ï¼šéªŒè¯ç”Ÿæˆçš„é‡ç°æµ‹è¯• ---
        if repro_test_success:
            print(f"  >> [STEP] Verify generated repro tests for {inst_id}", flush=True)
            for i in range(1): # åªéªŒè¯ç¬¬ 0 ä¸ª sampleï¼Œå› ä¸º Agentless --select é»˜è®¤æ‰¾ output_0
                repro_test_sample = os.path.join(repro_out, f"output_{i}_processed_reproduction_test.jsonl")
                if os.path.exists(repro_test_sample):
                    run_cmd(["python", "agentless/test/run_reproduction_tests.py", "--test_jsonl", repro_test_sample, "--run_id", f"v_{short_id}", "--dataset", "princeton-nlp/SWE-bench_Verified", "--instance_ids", str(orig_id), "--testing"], f"Verify Repro Test {i}", log)

        # 3.4 LLM ç­›é€‰æœ€ç»ˆé‡ç°æµ‹è¯•
        if repro_test_success:
            if not run_cmd(["python", "agentless/test/generate_reproduction_tests.py", "--max_samples", "40", "--output_folder", repro_out, "--output_file", "reproduction_tests.jsonl", "--select", "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", orig_id], "Select Final Repro Test", log):
                repro_test_success = False

        # 3.5 æ‰§è¡Œæµ‹è¯•å¹¶éªŒè¯è¡¥ä¸ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æµ‹è¯•æ•°é‡+å¹¶è¡Œï¼‰
        # åªæœ‰åœ¨ repro_test_success æ—¶æ‰æ‰§è¡Œæµ‹è¯•
        if repro_test_success:
            test_success = True
            reg_tests_file = os.path.join(reg_select_out, "output.jsonl")
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦æµ‹è¯•çš„è¡¥ä¸è·¯å¾„ - åªæµ‹è¯•å‰5ä¸ªÃ—4æ ·æœ¬=20ä¸ªï¼ˆåŸ40ä¸ªï¼‰
            test_tasks = []
            for i in range(4):
                folder = os.path.join(rep_path, f"sample_{i+1}")
                for num in range(min(5, 10)):  # åªæµ‹è¯•å‰5ä¸ª
                    pred_path = os.path.join(folder, f"output_{num}_processed.jsonl")
                    if os.path.exists(pred_path) and os.path.getsize(pred_path) > 0:
                        test_tasks.append((i, num, pred_path))
            
            print(f"  >> [INFO] Testing {len(test_tasks)} patches with max 8 parallel", flush=True)
            
            # å¹¶è¡Œæ‰§è¡Œæµ‹è¯•
            import time
            
            max_parallel = 8  # 8ä¸ªå¹¶è¡Œ
            running_processes = []
            
            for i, num, pred_path in test_tasks:
                # ç­‰å¾…ç›´åˆ°æœ‰ç©ºæ§½ä½
                while len(running_processes) >= max_parallel:
                    time.sleep(0.5)
                    running_processes = [p for p in running_processes if p.poll() is None]
                
                # å¯åŠ¨å›å½’æµ‹è¯•
                if os.path.exists(reg_tests_file) and os.path.getsize(reg_tests_file) > 0:
                    reg_log = os.path.join(inst_path, f"test_reg_{i}_{num}.log")
                    cmd = ["python", "agentless/test/run_regression_tests.py", "--regression_tests", reg_tests_file, "--predictions_path", pred_path, "--run_id", f"r_{short_id}_{i}_{num}", "--dataset", "princeton-nlp/SWE-bench_Verified", "--target_id", str(orig_id)]
                    with open(reg_log, "w") as f:
                        p = subprocess.Popen(cmd, env=env, cwd=AGENT_DIR, stdout=f, stderr=subprocess.STDOUT)
                        running_processes.append(p)
                
                # å¯åŠ¨é‡ç°æµ‹è¯•  
                if os.path.exists(repro_final) and os.path.getsize(repro_final) > 0:
                    repro_log = os.path.join(inst_path, f"test_repro_{i}_{num}.log")
                    cmd = ["python", "agentless/test/run_reproduction_tests.py", "--test_jsonl", repro_final, "--predictions_path", pred_path, "--run_id", f"p_{short_id}_{i}_{num}", "--dataset", "princeton-nlp/SWE-bench_Verified", "--instance_ids", str(orig_id)]
                    with open(repro_log, "w") as f:
                        p = subprocess.Popen(cmd, env=env, cwd=AGENT_DIR, stdout=f, stderr=subprocess.STDOUT)
                        running_processes.append(p)
            
            # ç­‰å¾…æ‰€æœ‰æµ‹è¯•å®Œæˆ
            if running_processes:
                print(f"  >> [INFO] Waiting for all tests to complete...", flush=True)
                max_wait_time = 3600  # æœ€å¤§ç­‰å¾…1å°æ—¶
                start_wait = time.time()
                check_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                last_progress_time = start_wait
                
                while running_processes:
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    elapsed = time.time() - start_wait
                    if elapsed > max_wait_time:
                        print(f"  !! [WARNING] Max wait time ({max_wait_time}s) exceeded. {len(running_processes)} processes may still be running.", flush=True)
                        break
                    
                    # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                    still_running = []
                    for p in running_processes:
                        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å·²é€€å‡º
                        if p.poll() is not None:
                            continue  # è¿›ç¨‹å·²é€€å‡ºï¼Œè·³è¿‡
                        
                        # è¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œç»§ç»­ç­‰å¾…
                        still_running.append(p)
                    
                    running_processes = still_running
                    
                    if running_processes:
                        time.sleep(check_interval)
                        # æ¯30ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
                        current_time = time.time()
                        if current_time - last_progress_time >= 30:
                            print(f"  >> [INFO] Still waiting for {len(running_processes)} processes (elapsed: {int(elapsed)}s)...", flush=True)
                            last_progress_time = current_time
                
                print(f"  >> [INFO] All tests completed", flush=True)

        # --- 4. æœ€ç»ˆé‡æ’åº (åŸºäºæµ‹è¯•ç»“æœ) ---
        rep_folders = ",".join([os.path.join(rep_path, f"sample_{i+1}") for i in range(4)])
        final_preds_src = os.path.join(AGENT_DIR, "all_preds.jsonl")
        final_preds_file = os.path.join(inst_path, "all_preds.jsonl")
        
        # å¼ºåˆ¶æ¸…ç†æ—§ç»“æœï¼Œç¡®ä¿"æ‰€è§å³æœ€æ–°"
        if os.path.exists(final_preds_src): os.remove(final_preds_src)
        if os.path.exists(final_preds_file): os.remove(final_preds_file)

        # åªæœ‰åœ¨ repro_test_success æ—¶æ‰å°è¯•å¸¦éªŒè¯çš„é‡æ’åº
        rerank_success = False
        if repro_test_success:
            # å°è¯•å¸¦éªŒè¯çš„é‡æ’åºï¼ˆ20åˆ†é’Ÿè¶…æ—¶ï¼‰
            rerank_success = run_cmd(
                ["python", "agentless/repair/rerank.py", "--patch_folder", rep_folders, "--num_samples", "40", "--deduplicate", "--regression", "--reproduction"], 
                "Final Reranking (w/ Test)", 
                log,
                timeout=1200  # 20åˆ†é’Ÿè¶…æ—¶
            )

            if not rerank_success or not os.path.exists(final_preds_src):
                print(f"  !! Reranking with test failed for {inst_id}. Falling back to simple reranking.")
                # é™çº§æ¨¡å¼ï¼šä¸å¸¦ --regression å’Œ --reproductionï¼ˆ20åˆ†é’Ÿè¶…æ—¶ï¼‰
                rerank_success = run_cmd(
                    ["python", "agentless/repair/rerank.py", "--patch_folder", rep_folders, "--num_samples", "40", "--deduplicate"], 
                    "Fallback Simple Reranking", 
                    log,
                    timeout=1200  # 20åˆ†é’Ÿè¶…æ—¶
                )

        # å¦‚æœrerankå¤±è´¥æˆ–repro_testå¤±è´¥ï¼Œfallbackä¸ºé»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„diff
        if not rerank_success or not os.path.exists(final_preds_src):
            print(f"  !! Reranking failed or skipped for {inst_id}. Falling back to default patch selection.")
            # ä»repairé˜¶æ®µçš„è¾“å‡ºä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„patch
            fallback_patch = None
            fallback_found = False
            
            # æŒ‰é¡ºåºæŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¯ç”¨çš„patchï¼šsample_1/output_0, sample_1/output_1, ..., sample_4/output_9
            for i in range(4):
                folder = os.path.join(rep_path, f"sample_{i+1}")
                for num in range(10):
                    proc_file = os.path.join(folder, f"output_{num}_processed.jsonl")
                    data = get_jsonl_last(proc_file)
                    if data:
                        patch = data.get("model_patch", "")
                        # æ£€æŸ¥ patch æ˜¯å¦éç©º
                        if patch and patch.strip():
                            fallback_patch = patch
                            fallback_found = True
                            print(f"  >> [FALLBACK] Selected patch from sample_{i+1}/output_{num}", flush=True)
                            break
                if fallback_found:
                    break
            
            # åˆ›å»ºfallbackçš„all_preds.jsonlï¼ˆå³ä½¿æ²¡æœ‰æ‰¾åˆ°patchä¹Ÿè¦åˆ›å»ºï¼Œè®°å½•å¤±è´¥åŸå› ï¼‰
            fallback_reason = "rerank_timeout_or_failed" if (rerank_success == False and repro_test_success) else ("repro_test_failed_or_timeout" if not repro_test_success else "test_phase_failed")
            fallback_result = {
                "model_patch": fallback_patch if fallback_patch else "",
                "fallback_reason": fallback_reason,
                "patch_found": fallback_found
            }
            with open(final_preds_file, "w") as f:
                f.write(json.dumps(fallback_result) + "\n")
            if fallback_patch:
                print(f"  >> [FALLBACK] Created fallback all_preds.jsonl with default patch", flush=True)
            else:
                print(f"  !! [WARNING] No valid patches found for fallback. Created all_preds.jsonl with empty patch.", flush=True)
        else:
            # å¦‚æœrerankæˆåŠŸï¼Œç§»åŠ¨ç»“æœæ–‡ä»¶
            shutil.move(final_preds_src, final_preds_file)
    
    finally:
        # --- 5. è½¨è¿¹èšåˆ (Traj) - æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½è®°å½• ---
        print(f"  >> [TRAJ] Saving trajectory data for {inst_id}", flush=True)
        traj = {
            "instance_id": inst_id,
            "original_id": orig_id,
            "1_model_selected_files": (get_jsonl_last(f_loc_file) or {}).get("found_files", []) if f_loc_file else [],
            "2_embedding_selected_files": (get_jsonl_last(ret_file) or {}).get("found_files", []) if ret_file else [],
            "3_final_combined_files": (get_jsonl_last(comb_file_path) or {}).get("found_files", []) if comb_file_path else [],
            "4_related_elements": (get_jsonl_last(r_loc_file) or {}).get("found_related_locs", {}) if r_loc_file else {},
            "5_sampled_edit_locs_and_patches": [],
            "6_final_selected_patch": None
        }
        
        # æ”¶é›†é‡‡æ ·è¡¥ä¸æ˜ å°„ (ä» processed æ–‡ä»¶ä¸­è¯»å–)
        if m_out:
            for i in range(4):
                folder = os.path.join(rep_path, f"sample_{i+1}")
                all_patches_for_sample = []
                edit_locs_for_sample = []
                
                # å°è¯•ä»è¯¥ sample çš„æ‰€æœ‰ 10 ä¸ª processed æ–‡ä»¶ä¸­æ”¶é›†è¡¥ä¸
                # ä¿æŒåˆ—è¡¨é•¿åº¦ä¸º 10ï¼Œç”¨ None è¡¨ç¤ºç¼ºå¤±æˆ–ç©ºçš„è¡¥ä¸
                for num in range(10):
                    proc_file = os.path.join(folder, f"output_{num}_processed.jsonl")
                    data = get_jsonl_last(proc_file)
                    if data:
                        patch = data.get("model_patch", "")
                        # åªæ·»åŠ éç©ºçš„è¡¥ä¸ï¼Œç©ºå­—ç¬¦ä¸²ç”¨ None è¡¨ç¤º
                        all_patches_for_sample.append(patch if patch and patch.strip() else None)
                    else:
                        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç”¨ None è¡¨ç¤º
                        all_patches_for_sample.append(None)
                
                # è·å–è¯¥ sample çš„ edit_locs
                sample_loc_file = os.path.join(m_out, f"loc_merged_{i}-{i}_outputs.jsonl")
                loc_data = get_jsonl_last(sample_loc_file)
                if loc_data:
                    edit_locs_for_sample = loc_data.get("found_edit_locs", [])

                traj["5_sampled_edit_locs_and_patches"].append({
                    "sample_index": i, 
                    "edit_locs": edit_locs_for_sample, 
                    "patches": all_patches_for_sample
                })

        # æœ€ç»ˆé€‰ä¸­çš„è¡¥ä¸
        final_res = get_jsonl_last(final_preds_file)
        if final_res:
            traj["6_final_selected_patch"] = final_res.get("model_patch")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(TRAJS_DIR, exist_ok=True)
        with open(traj_file, "w") as f:
            json.dump(traj, f, indent=4)
        
        print(f"  >> [TRAJ] Trajectory saved to {traj_file}", flush=True)
    
    return True

def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œ Agentless Verified å·¥ä½œæµ")
    parser.add_argument(
        "--instance_id", 
        type=str, 
        default=None,
        help="æŒ‡å®šè¦è¿è¡Œçš„å•ä¸ªå®ä¾‹IDï¼ˆä¾‹å¦‚: SWE-Bench-Verified__python__maintenance__bugfix__27320d49ï¼‰ã€‚å¦‚æœä¸æŒ‡å®šï¼Œåˆ™è¿è¡Œæ‰€æœ‰å®ä¾‹ã€‚"
    )
    parser.add_argument(
        "--original_id",
        type=str,
        default=None,
        help="æŒ‡å®šè¦è¿è¡Œçš„åŸå§‹å®ä¾‹IDï¼ˆä¾‹å¦‚: scikit-learn__scikit-learn-25232ï¼‰ã€‚å¦‚æœæŒ‡å®šäº† instance_idï¼Œæ­¤å‚æ•°ä¼šè¢«å¿½ç•¥ã€‚"
    )
    
    args = parser.parse_args()
    
    os.makedirs(DETAILS_DIR, exist_ok=True)
    os.makedirs(TRAJS_DIR, exist_ok=True)
    df = pd.read_csv(DATA_FILE)
    
    # å¦‚æœæŒ‡å®šäº† instance_idï¼Œåªå¤„ç†è¯¥å®ä¾‹
    if args.instance_id:
        filtered_df = df[df['instance_id'] == args.instance_id]
        if len(filtered_df) == 0:
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å®ä¾‹ID '{args.instance_id}'")
            print(f"å¯ç”¨çš„å®ä¾‹IDç¤ºä¾‹:")
            print(df['instance_id'].head(5).to_string(index=False))
            return
        print(f"ğŸ¯ å•å®ä¾‹è¿è¡Œæ¨¡å¼: {args.instance_id}")
        for _, row in filtered_df.iterrows():
            process_instance(row)
    # å¦‚æœæŒ‡å®šäº† original_idï¼Œåªå¤„ç†è¯¥å®ä¾‹
    elif args.original_id:
        filtered_df = df[df['original_inst_id'] == args.original_id]
        if len(filtered_df) == 0:
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åŸå§‹å®ä¾‹ID '{args.original_id}'")
            print(f"å¯ç”¨çš„åŸå§‹å®ä¾‹IDç¤ºä¾‹:")
            print(df['original_inst_id'].head(5).to_string(index=False))
            return
        print(f"ğŸ¯ å•å®ä¾‹è¿è¡Œæ¨¡å¼ (é€šè¿‡ original_id): {args.original_id}")
        for _, row in filtered_df.iterrows():
            process_instance(row)
    # å¦åˆ™å¤„ç†æ‰€æœ‰å®ä¾‹
    else:
        print("ğŸ“‹ æ‰¹é‡è¿è¡Œæ¨¡å¼: å¤„ç†æ‰€æœ‰å®ä¾‹")
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Verified Full Workflow"):
            process_instance(row)

if __name__ == "__main__":
    main()
