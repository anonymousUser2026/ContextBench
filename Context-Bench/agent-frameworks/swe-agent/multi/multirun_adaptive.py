#!/usr/bin/env python3
"""
è‡ªé€‚åº”å¤šå®ä¾‹è¿è¡Œè„šæœ¬
- åŠ¨æ€æ£€æµ‹å®¹å™¨å ç”¨
- é‡åˆ°å†²çªè‡ªåŠ¨è·³è¿‡ï¼Œæ¢ä¸‹ä¸€ä¸ªå®ä¾‹
- ç›´åˆ°æ‰€æœ‰å®ä¾‹éƒ½å®Œæˆæˆ–æ— æ³•è¿è¡Œ
"""

from __future__ import annotations

import logging
import json
import os
import re
import subprocess
import traceback
import time
import threading
import copy
from typing import Any, Set, Dict
from dataclasses import dataclass
from getpass import getuser
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import yaml
import docker
from rich.markdown import Markdown

try:
    from rich_argparse import RichHelpFormatter
except ImportError:
    msg = "Please install the rich_argparse package with `pip install rich_argparse`."
    raise ImportError(msg)

from simple_parsing import parse
from simple_parsing.helpers.flatten import FlattenedAccess
from simple_parsing.helpers.serialization.serializable import FrozenSerializable
from swebench import KEY_INSTANCE_ID, KEY_MODEL, KEY_PREDICTION
from multi_swe_bench.harness.build_dataset import CliArgs
from unidiff import PatchSet

from sweagent import CONFIG_DIR
from sweagent.utils.log import get_logger
from sweagent.agent.agents import Agent, AgentArguments
from sweagent.agent.models import ModelArguments
from sweagent.environment.swe_env import EnvironmentArguments, SWEEnv
from sweagent.environment.utils import get_instances

logger = get_logger("swe-agent-run-adaptive")
logging.getLogger("simple_parsing").setLevel(logging.WARNING)

INSTANCE_LOG_DIR = 'logs/'

# å…¨å±€é”å’ŒçŠ¶æ€ç®¡ç†
_lock = threading.Lock()
_running_images: Set[str] = set()  # æ­£åœ¨è¿è¡Œçš„é•œåƒ
_completed_instances: Set[str] = set()  # å·²å®Œæˆçš„å®ä¾‹
_failed_instances: Dict[str, str] = {}  # å¤±è´¥çš„å®ä¾‹åŠåŸå› 
_skipped_count = 0  # å› å†²çªè·³è¿‡çš„æ¬¡æ•°


def get_image_name_from_instance_id(instance_id: str, all_datas: dict) -> str:
    """ä»å®ä¾‹IDè·å–é•œåƒå"""
    if instance_id in all_datas:
        record = all_datas[instance_id]
        # é•œåƒåæ ¼å¼: org/repo:pr-xxx
        org_repo = instance_id.rsplit('-', 1)[0].replace('__', '/')
        # ä» record ä¸­è·å– base_commit æˆ– pr ä¿¡æ¯
        if hasattr(record, 'instance') and hasattr(record.instance, 'pr'):
            pr = record.instance.pr
            if hasattr(pr, 'base_commit'):
                return f"{org_repo}:pr-{pr.base_commit[:7]}"
        # å›é€€ï¼šä½¿ç”¨å®ä¾‹IDæ¨æ–­
        parts = instance_id.rsplit('-', 1)
        if len(parts) == 2:
            return f"{org_repo}:pr-{parts[1]}"
    return instance_id


def is_image_in_use(image_name: str) -> bool:
    """æ£€æŸ¥é•œåƒæ˜¯å¦æ­£åœ¨è¢«ä½¿ç”¨ï¼ˆæœ‰è¿è¡Œä¸­çš„å®¹å™¨ï¼‰"""
    try:
        client = docker.from_env(timeout=10)
        containers = client.containers.list()
        for container in containers:
            if container.image.tags:
                for tag in container.image.tags:
                    if image_name in tag or tag in image_name:
                        return True
            # ä¹Ÿæ£€æŸ¥å®¹å™¨åæ˜¯å¦åŒ…å«é•œåƒåçš„ç‰¹å¾
            container_name = container.name
            image_sanitized = image_name.replace("/", "-").replace(":", "-")
            if image_sanitized in container_name:
                return True
        return False
    except Exception as e:
        logger.warning(f"æ£€æŸ¥é•œåƒä½¿ç”¨çŠ¶æ€å¤±è´¥: {e}")
        return False  # å¤±è´¥æ—¶å‡è®¾ä¸åœ¨ä½¿ç”¨


def is_instance_available(instance_id: str, all_datas: dict) -> bool:
    """æ£€æŸ¥å®ä¾‹æ˜¯å¦å¯ç”¨ï¼ˆæ²¡æœ‰è¢«å ç”¨ï¼‰"""
    global _running_images, _completed_instances
    
    with _lock:
        # å·²å®Œæˆ
        if instance_id in _completed_instances:
            return False
        
        # å·²å¤±è´¥
        if instance_id in _failed_instances:
            return False
    
    # è·å–é•œåƒå
    image_name = get_image_name_from_instance_id(instance_id, all_datas)
    
    with _lock:
        # æ£€æŸ¥æœ¬è¿›ç¨‹æ˜¯å¦æ­£åœ¨ä½¿ç”¨è¿™ä¸ªé•œåƒ
        if image_name in _running_images:
            return False
    
    # æ£€æŸ¥å¤–éƒ¨è¿›ç¨‹æ˜¯å¦æ­£åœ¨ä½¿ç”¨è¿™ä¸ªé•œåƒ
    if is_image_in_use(image_name):
        return False
    
    return True


def mark_instance_running(instance_id: str, all_datas: dict):
    """æ ‡è®°å®ä¾‹ä¸ºè¿è¡Œä¸­"""
    global _running_images
    image_name = get_image_name_from_instance_id(instance_id, all_datas)
    with _lock:
        _running_images.add(image_name)


def mark_instance_done(instance_id: str, all_datas: dict, success: bool, error: str = ""):
    """æ ‡è®°å®ä¾‹ä¸ºå®Œæˆ"""
    global _running_images, _completed_instances, _failed_instances
    image_name = get_image_name_from_instance_id(instance_id, all_datas)
    with _lock:
        _running_images.discard(image_name)
        if success:
            _completed_instances.add(instance_id)
        else:
            _failed_instances[instance_id] = error


@dataclass(frozen=True)
class ActionsArguments(FlattenedAccess, FrozenSerializable):
    open_pr: bool = False
    apply_patch_locally: bool = False
    skip_if_commits_reference_issue: bool = True
    push_gh_repo_url: str = ""

    def __post_init__(self):
        if self.push_gh_repo_url:
            msg = "push_gh_repo_url is obsolete. Use repo_path instead"
            raise ValueError(msg)


@dataclass(frozen=True)
class ScriptArguments(FlattenedAccess, FrozenSerializable):
    environment: EnvironmentArguments
    agent: AgentArguments
    actions: ActionsArguments
    instance_filter: str = ".*"
    skip_existing: bool = True
    suffix: str = ""
    raise_exceptions: bool = False
    print_config: bool = True

    @property
    def run_name(self):
        model_name = self.agent.model.model_name.replace(":", "-")
        from sweagent.environment.utils import get_data_path_name
        data_stem = get_data_path_name(str(self.environment.cli_args.pr_file))
        config_stem = Path(self.agent.config_file).stem
        temp = self.agent.model.temperature
        top_p = self.agent.model.top_p
        per_instance_cost_limit = self.agent.model.per_instance_cost_limit
        install_env = self.environment.install_environment
        return (
            f"{model_name}__{data_stem}__{config_stem}__t-{temp:.2f}__p-{top_p:.2f}"
            + f"__c-{per_instance_cost_limit:.2f}__install-{int(install_env)}"
            + (f"__{self.suffix}" if self.suffix else "")
        )


class _ContinueLoop(Exception):
    pass


class Main:
    def __init__(self, args: ScriptArguments, filter_instance: str):
        self.args = args
        self.instance_id = filter_instance
        self.traj_dir = Path("trajectories") / Path(getuser()) / args.run_name
        self.traj_dir.mkdir(parents=True, exist_ok=True)
        if self.should_skip(self.instance_id):
            raise _ContinueLoop
        log_dir = Path(INSTANCE_LOG_DIR) / args.run_name / self.instance_id
        if log_dir.exists():
            file_path = log_dir / "log"
            file_path.unlink(missing_ok=True)
        self.agent = Agent("primary", args.agent, log_dir)
        self.env = SWEEnv(args.environment, log_dir)

    def run(self):
        assert isinstance(self.instance_id, str)
        if self.should_skip(self.instance_id):
            raise _ContinueLoop
        logger.info("â–¶ï¸  Beginning task " + self.instance_id)
        observation, info = self.env.reset(self.instance_id)
        if info is None:
            raise _ContinueLoop

        issue = getattr(self.env, "query", None)
        files = []
        if self.env.record.instance.pr.fix_patch:
            files = "\n".join([f"- {x.path}" for x in PatchSet(self.env.record.instance.pr.fix_patch).modified_files])
        test_files = []
        if self.env.record.instance.pr.test_patch:
            test_patch_obj = PatchSet(self.env.record.instance.pr.test_patch)
            test_files = "\n".join([f"- {x.path}" for x in test_patch_obj.modified_files + test_patch_obj.added_files])
        tests = ""

        setup_args = {"issue": issue, "files": files, "test_files": test_files, "tests": tests}
        info, trajectory = self.agent.run(
            setup_args=setup_args,
            env=self.env,
            observation=observation,
            traj_dir=self.traj_dir,
            return_type="info_trajectory",
        )
        self._save_predictions(self.instance_id, info)
        self._save_patch(self.instance_id, info)

    def main(self):
        logger.info(f'running the instance id {self.instance_id} now!')
        try:
            self.run()
        except _ContinueLoop:
            logger.info("Skipping instance")
        except KeyboardInterrupt:
            logger.info("Exiting...")
            self.env.close()
        except SystemExit:
            logger.critical("âŒ Exiting because SystemExit was called")
            self.env.close()
            raise
        except Exception as e:
            traceback.print_exc()
            if self.args.raise_exceptions:
                self.env.close()
                raise e
            if self.env.record:
                logger.warning(f"âŒ Failed on {self.env.record.data['instance_id']}: {e}")
            else:
                logger.warning("âŒ Failed on unknown instance")
            raise

    def should_skip(self, instance_id: str) -> bool:
        if re.match(self.args.instance_filter, instance_id) is None:
            return True
        if not self.args.skip_existing:
            return False
        log_path = self.traj_dir / (instance_id + ".traj")
        if log_path.exists():
            with log_path.open("r") as f:
                data = json.load(f)
            exit_status = data["info"].get("exit_status", None)
            if exit_status == "early_exit" or exit_status is None:
                os.remove(log_path)
            else:
                logger.info(f"â­ï¸ Skipping existing trajectory: {log_path}")
                return True
        return False

    def _save_predictions(self, instance_id: str, info):
        output_file = self.traj_dir / "all_preds.jsonl"
        model_patch = info["submission"] if "submission" in info else None
        datum = {
            KEY_MODEL: Path(self.traj_dir).name,
            KEY_INSTANCE_ID: instance_id,
            KEY_PREDICTION: model_patch,
        }
        with open(output_file, "a+") as fp:
            print(json.dumps(datum), file=fp, flush=True)
        logger.info(f"Saved predictions to {output_file}")

    def _save_patch(self, instance_id: str, info):
        patch_output_dir = self.traj_dir / "patches"
        patch_output_dir.mkdir(exist_ok=True, parents=True)
        patch_output_file = patch_output_dir / f"{instance_id}.patch"
        if info.get("submission"):
            patch_output_file.write_text(info["submission"])
            logger.info(f"ğŸ’¾ Trajectory saved for {instance_id}")


def get_args(args=None) -> ScriptArguments:
    defaults = ScriptArguments(
        suffix="",
        environment=EnvironmentArguments(
            cli_args=CliArgs(
                workdir=Path("data_files"),
                repo_dir=None,
                pr_file='data/',
                need_clone=True,
                max_workers_build_image=64,
                max_workers_run_instance=64,
                clear_env=False,
                global_env=[],
            ),
            verbose=True,
            install_environment=True,
            cache_task_images=False,
        ),
        skip_existing=True,
        agent=AgentArguments(
            model=ModelArguments(
                model_name="gpt4",
                total_cost_limit=0.0,
                per_instance_cost_limit=3.0,
                temperature=0.0,
                top_p=0.95,
            ),
            config_file=CONFIG_DIR / "default.yaml",
        ),
        actions=ActionsArguments(open_pr=False, skip_if_commits_reference_issue=True),
    )

    yaml.add_representer(str, lambda dumper, data: 
        dumper.represent_scalar("tag:yaml.org,2002:str", data, style="|") 
        if data.count("\n") > 0 else dumper.represent_scalar("tag:yaml.org,2002:str", data))

    return parse(
        ScriptArguments,
        default=defaults,
        add_config_path_arg=False,
        args=args,
        formatter_class=RichHelpFormatter,
        description=Markdown("Adaptive multi-instance runner"),
    )


def run_single_adaptive(scripts, instance_id: str, all_datas: dict, max_retries: int = 3):
    """è¿è¡Œå•ä¸ªå®ä¾‹ï¼Œæ”¯æŒå†²çªæ£€æµ‹å’Œé‡è¯•"""
    global _skipped_count
    
    for attempt in range(max_retries):
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨
        if not is_instance_available(instance_id, all_datas):
            with _lock:
                _skipped_count += 1
            logger.info(f"â³ å®ä¾‹ {instance_id} è¢«å ç”¨ï¼Œè·³è¿‡ (attempt {attempt + 1})")
            time.sleep(2)  # ç­‰å¾…ä¸€ä¸‹å†æ£€æŸ¥
            continue
        
        # æ ‡è®°ä¸ºè¿è¡Œä¸­
        mark_instance_running(instance_id, all_datas)
        
        try:
            copy_args = copy.deepcopy(scripts)
            handler = Main(copy_args, instance_id)
            handler.main()
            mark_instance_done(instance_id, all_datas, success=True)
            logger.info(f"âœ… å®Œæˆå®ä¾‹ {instance_id}")
            return True
        except _ContinueLoop:
            mark_instance_done(instance_id, all_datas, success=True, error="skipped")
            logger.info(f"â­ï¸ å®ä¾‹ {instance_id} å·²è·³è¿‡")
            return True
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯å®¹å™¨å†²çªé”™è¯¯
            if "container" in error_msg.lower() or "conflict" in error_msg.lower():
                mark_instance_done(instance_id, all_datas, success=False, error="conflict")
                logger.warning(f"ğŸ”„ å®ä¾‹ {instance_id} é‡åˆ°å®¹å™¨å†²çªï¼Œå°†é‡è¯•")
                time.sleep(5)
                continue
            else:
                mark_instance_done(instance_id, all_datas, success=False, error=error_msg[:100])
                logger.error(f"âŒ å®ä¾‹ {instance_id} å¤±è´¥: {error_msg[:100]}")
                return False
    
    # é‡è¯•æ¬¡æ•°ç”¨å®Œ
    mark_instance_done(instance_id, all_datas, success=False, error="max_retries")
    return False


def main(args: ScriptArguments):
    global _completed_instances, _failed_instances, _skipped_count
    
    running_threads = int(os.environ.get('RUNNING_THREADS', '50'))
    logger.info(f"ğŸš€ å¯åŠ¨è‡ªé€‚åº”å¤šå®ä¾‹è¿è¡Œå™¨ï¼Œå¹¶å‘æ•°: {running_threads}")
    
    cli_args = args.environment.cli_args
    all_datas = get_instances(
        cli_args.pr_file,
        cli_args,
        prebuild=args.environment.pre_build_all_images,
    )
    instance_ids = list(all_datas.keys())
    total_instances = len(instance_ids)
    logger.info(f"ğŸ“Š æ€»å®ä¾‹æ•°: {total_instances}")
    
    post_args = parse(
        ScriptArguments,
        default=args,
        add_config_path_arg=False,
        args=['--pre_build_all_images=False'],
        formatter_class=RichHelpFormatter,
        description=Markdown("Adaptive runner"),
    )
    
    executor = ThreadPoolExecutor(max_workers=running_threads)
    futures = {
        executor.submit(run_single_adaptive, post_args, instance_id, all_datas): instance_id 
        for instance_id in instance_ids
    }
    
    # å®šæœŸæ‰“å°è¿›åº¦
    start_time = time.time()
    completed = 0
    
    for future in as_completed(futures):
        instance_id = futures[future]
        try:
            result = future.result()
            completed += 1
            elapsed = time.time() - start_time
            remaining = total_instances - completed
            rate = completed / elapsed if elapsed > 0 else 0
            eta = remaining / rate if rate > 0 else 0
            
            logger.info(
                f"ğŸ“ˆ è¿›åº¦: {completed}/{total_instances} "
                f"({100*completed/total_instances:.1f}%) "
                f"| æˆåŠŸ: {len(_completed_instances)} "
                f"| å¤±è´¥: {len(_failed_instances)} "
                f"| è·³è¿‡: {_skipped_count} "
                f"| ETA: {eta/60:.1f}min"
            )
        except Exception as e:
            logger.error(f"âŒ å®ä¾‹ {instance_id} å¼‚å¸¸: {e}")
    
    # æœ€ç»ˆç»Ÿè®¡
    logger.info("=" * 50)
    logger.info(f"ğŸ è¿è¡Œå®Œæˆ!")
    logger.info(f"   æ€»å®ä¾‹: {total_instances}")
    logger.info(f"   æˆåŠŸ: {len(_completed_instances)}")
    logger.info(f"   å¤±è´¥: {len(_failed_instances)}")
    logger.info(f"   å†²çªè·³è¿‡æ¬¡æ•°: {_skipped_count}")
    
    if _failed_instances:
        logger.info("å¤±è´¥å®ä¾‹:")
        for inst, err in list(_failed_instances.items())[:10]:
            logger.info(f"  - {inst}: {err}")


if __name__ == "__main__":
    main(get_args())
















